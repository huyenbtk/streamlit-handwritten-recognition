{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[{"sourceId":1400106,"sourceType":"datasetVersion","datasetId":818027},{"sourceId":62997,"sourceType":"modelInstanceVersion","modelInstanceId":52570},{"sourceId":63032,"sourceType":"modelInstanceVersion","modelInstanceId":52598}],"dockerImageVersionId":30698,"isInternetEnabled":false,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import os\nimport cv2\nimport random\nimport numpy as np\nimport pandas as pd\nfrom PIL import Image\nimport matplotlib.pyplot as plt\nimport gc\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.nn import CTCLoss\nimport torch.optim as optim\nfrom torch.utils.data import Dataset, DataLoader\nfrom torchvision import transforms\nimport math","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:22.826095Z","iopub.execute_input":"2024-06-08T17:36:22.82645Z","iopub.status.idle":"2024-06-08T17:36:30.340652Z","shell.execute_reply.started":"2024-06-08T17:36:22.826419Z","shell.execute_reply":"2024-06-08T17:36:30.339772Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Load data","metadata":{}},{"cell_type":"code","source":"train = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_train_v2.csv')\nvalid = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_validation_v2.csv')\ntest = pd.read_csv('/kaggle/input/handwriting-recognition/written_name_test_v2.csv')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:30.342173Z","iopub.execute_input":"2024-06-08T17:36:30.342857Z","iopub.status.idle":"2024-06-08T17:36:30.881832Z","shell.execute_reply.started":"2024-06-08T17:36:30.342828Z","shell.execute_reply":"2024-06-08T17:36:30.88098Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_img_dir = '/kaggle/input/handwriting-recognition/train_v2/train/'\nval_img_dir = '/kaggle/input/handwriting-recognition/validation_v2/validation/'\ntest_img_dir = '/kaggle/input/handwriting-recognition/test_v2/test'","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:30.882945Z","iopub.execute_input":"2024-06-08T17:36:30.883234Z","iopub.status.idle":"2024-06-08T17:36:30.888794Z","shell.execute_reply.started":"2024-06-08T17:36:30.883211Z","shell.execute_reply":"2024-06-08T17:36:30.887449Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# View & Clean data","metadata":{}},{"cell_type":"markdown","source":"## Overview","metadata":{}},{"cell_type":"code","source":"train.head(6)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:30.89169Z","iopub.execute_input":"2024-06-08T17:36:30.892239Z","iopub.status.idle":"2024-06-08T17:36:30.914636Z","shell.execute_reply.started":"2024-06-08T17:36:30.892201Z","shell.execute_reply":"2024-06-08T17:36:30.913717Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"plt.figure(figsize=(15, 10))\nfor i in range(6):\n    ax = plt.subplot(2, 3, i+1)\n    img_dir = train_img_dir +train.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    plt.imshow(image, cmap = 'gray')\n    plt.title(train.loc[i, 'IDENTITY'], fontsize=12)\n    plt.axis('off')\n\nplt.subplots_adjust(wspace=0.2, hspace=-0.8)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:30.915783Z","iopub.execute_input":"2024-06-08T17:36:30.916073Z","iopub.status.idle":"2024-06-08T17:36:31.537333Z","shell.execute_reply.started":"2024-06-08T17:36:30.916049Z","shell.execute_reply":"2024-06-08T17:36:31.53628Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Missing value","metadata":{}},{"cell_type":"code","source":"print(f\"Training set missing values: {train['IDENTITY'].isnull().sum()}\")\nprint(f\"Validation set missing values: {valid['IDENTITY'].isnull().sum()}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:31.538602Z","iopub.execute_input":"2024-06-08T17:36:31.538911Z","iopub.status.idle":"2024-06-08T17:36:31.581664Z","shell.execute_reply.started":"2024-06-08T17:36:31.538885Z","shell.execute_reply":"2024-06-08T17:36:31.58072Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train.dropna(axis=0, inplace=True)\nvalid.dropna(axis=0, inplace=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:31.582895Z","iopub.execute_input":"2024-06-08T17:36:31.583195Z","iopub.status.idle":"2024-06-08T17:36:31.677953Z","shell.execute_reply.started":"2024-06-08T17:36:31.583171Z","shell.execute_reply":"2024-06-08T17:36:31.677077Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Special value","metadata":{}},{"cell_type":"code","source":"train['IDENTITY'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:31.679079Z","iopub.execute_input":"2024-06-08T17:36:31.679355Z","iopub.status.idle":"2024-06-08T17:36:31.973904Z","shell.execute_reply.started":"2024-06-08T17:36:31.679332Z","shell.execute_reply":"2024-06-08T17:36:31.972911Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of unreadable images: {train[train['IDENTITY'] == 'UNREADABLE'].value_counts().sum()}\")\nprint(f\"Number of empty images: {train[train['IDENTITY'] == 'EMPTY'].value_counts().sum()}\")\nlowercase_count = train[train['IDENTITY'].str.contains(r'[a-z]', regex=True)].shape[0]\nprint(f\"Number of labels with lowercase: {lowercase_count}\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:31.975209Z","iopub.execute_input":"2024-06-08T17:36:31.975602Z","iopub.status.idle":"2024-06-08T17:36:32.290025Z","shell.execute_reply.started":"2024-06-08T17:36:31.975569Z","shell.execute_reply":"2024-06-08T17:36:32.288982Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"unreadable = train[train['IDENTITY'] == 'UNREADABLE']\nempty = train[train['IDENTITY'] == 'EMPTY']\n\nunreadable.reset_index(inplace=True, drop=True)\nempty.reset_index(inplace=True, drop=True)\n\nplt.figure(figsize=(15, 10))\n\nfor i in range(3):\n    # Plot unreadable images\n    ax = plt.subplot(2, 3, i+1)\n    img_dir = train_img_dir + unreadable.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    plt.imshow(image, cmap='gray')\n    plt.title(unreadable.loc[i, 'IDENTITY'], fontsize=12)\n    plt.axis('off')\n\n    # Plot empty images\n    ax = plt.subplot(2, 3, i+4)\n    img_dir = train_img_dir + empty.loc[i, 'FILENAME']\n    image = cv2.imread(img_dir, cv2.IMREAD_GRAYSCALE)\n    plt.imshow(image, cmap='gray')\n    plt.title(empty.loc[i, 'IDENTITY'], fontsize=12)\n    plt.axis('off')\n\nplt.subplots_adjust(wspace=0.2, hspace=-0.8)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:32.293593Z","iopub.execute_input":"2024-06-08T17:36:32.293931Z","iopub.status.idle":"2024-06-08T17:36:32.959301Z","shell.execute_reply.started":"2024-06-08T17:36:32.293906Z","shell.execute_reply":"2024-06-08T17:36:32.958343Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = train[train['IDENTITY'] != 'UNREADABLE']\ntrain = train[train['IDENTITY'] != 'EMPTY']\ntrain['IDENTITY'] = train['IDENTITY'].str.upper()\ntrain.reset_index(inplace = True, drop=True) ","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:32.960501Z","iopub.execute_input":"2024-06-08T17:36:32.960813Z","iopub.status.idle":"2024-06-08T17:36:33.227839Z","shell.execute_reply.started":"2024-06-08T17:36:32.960786Z","shell.execute_reply":"2024-06-08T17:36:33.226833Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"valid = valid[valid['IDENTITY'] != 'UNREADABLE']\nvalid = valid[valid['IDENTITY'] != 'EMPTY']\nvalid['IDENTITY'] = valid['IDENTITY'].str.upper()\nvalid.reset_index(inplace = True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.229091Z","iopub.execute_input":"2024-06-08T17:36:33.229406Z","iopub.status.idle":"2024-06-08T17:36:33.268284Z","shell.execute_reply.started":"2024-06-08T17:36:33.229378Z","shell.execute_reply":"2024-06-08T17:36:33.267266Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Custom dataset","metadata":{}},{"cell_type":"code","source":"class dataset(Dataset):\n    def __init__(self, img_dir, filenames, labels, transform=None):\n        self.img_dir = img_dir\n        self.filenames = filenames\n        self.labels = labels\n        self.transform = transform\n        self.alphabet = u\"ABCDEFGHIJKLMNOPQRSTUVWXYZ-'` \"\n        self.char_to_idx = {char: idx + 1 for idx, char in enumerate(self.alphabet)}\n        self.char_to_idx['<BLANK>'] = 0\n\n    def __len__(self):\n        return len(self.filenames)\n    \n    def encode_label(self, label):\n        return [self.char_to_idx[char] for char in label]\n    def __getitem__(self, idx):\n        img_path = os.path.join(self.img_dir, self.filenames[idx])\n        image = Image.open(img_path).convert('L')  # Convert image to grayscale\n        if self.transform:\n            image = self.transform(image)\n        label = self.labels[idx]\n        label_encoded = self.encode_label(label)\n        label_length = len(label_encoded)\n\n        return image, torch.tensor(label_encoded, dtype=torch.long), torch.tensor(label_length, dtype=torch.long)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.269632Z","iopub.execute_input":"2024-06-08T17:36:33.270026Z","iopub.status.idle":"2024-06-08T17:36:33.281121Z","shell.execute_reply.started":"2024-06-08T17:36:33.26999Z","shell.execute_reply":"2024-06-08T17:36:33.2801Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PreprocessTransform:\n    def __call__(self, img):\n        img = np.array(img)\n        (h, w) = img.shape\n        final_img = np.ones([64, 256]) * 255  # blank white image\n\n        # crop\n        if w > 256:\n            img = img[:, :256]\n\n        if h > 64:\n            img = img[:64, :]\n\n        final_img[:h, :w] = img\n        final_img = cv2.rotate(final_img, cv2.ROTATE_90_CLOCKWISE)\n        \n        # Convert to PyTorch tensor and normalize to [0, 1]\n        final_img = torch.tensor(final_img, dtype=torch.float32) / 255.0\n        final_img = final_img.unsqueeze(0)  # Add channel dimension\n\n        return final_img\ntransform = PreprocessTransform()","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.282369Z","iopub.execute_input":"2024-06-08T17:36:33.282702Z","iopub.status.idle":"2024-06-08T17:36:33.293368Z","shell.execute_reply.started":"2024-06-08T17:36:33.282674Z","shell.execute_reply":"2024-06-08T17:36:33.292398Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Collate function to handle variable-length sequences\ndef collate_fn(batch):\n    images, labels, label_lengths = zip(*batch)\n    \n    # Pad the labels\n    max_length = max([len(label) for label in labels])\n    padded_labels = torch.zeros(len(labels), max_length, dtype=torch.long)\n    \n    for i, label in enumerate(labels):\n        padded_labels[i, :len(label)] = label\n    \n    images = torch.stack(images, 0)\n    label_lengths = torch.stack(label_lengths, 0)\n    \n    return images, padded_labels, label_lengths","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.294637Z","iopub.execute_input":"2024-06-08T17:36:33.295039Z","iopub.status.idle":"2024-06-08T17:36:33.303657Z","shell.execute_reply.started":"2024-06-08T17:36:33.295004Z","shell.execute_reply":"2024-06-08T17:36:33.302681Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.304874Z","iopub.execute_input":"2024-06-08T17:36:33.30556Z","iopub.status.idle":"2024-06-08T17:36:33.316884Z","shell.execute_reply.started":"2024-06-08T17:36:33.305524Z","shell.execute_reply":"2024-06-08T17:36:33.315983Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_size = int(len(train))\ntrain_dataset = dataset(train_img_dir, train['FILENAME'].head(train_size).values, train['IDENTITY'].head(train_size).values, transform=transform)\ntrain_loader = DataLoader(train_dataset, batch_size=1024, shuffle=True, num_workers=3, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.318138Z","iopub.execute_input":"2024-06-08T17:36:33.318435Z","iopub.status.idle":"2024-06-08T17:36:33.325361Z","shell.execute_reply.started":"2024-06-08T17:36:33.318409Z","shell.execute_reply":"2024-06-08T17:36:33.324555Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Model","metadata":{}},{"cell_type":"code","source":"class CNNtoRNN(nn.Module):\n    def __init__(self, num_of_characters):\n        super(CNNtoRNN, self).__init__()\n        \n        # Convolutional layers\n        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)  # (3, 3) kernel, 'same' padding\n        self.bn1 = nn.BatchNorm2d(32)\n        \n        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n        self.bn2 = nn.BatchNorm2d(64)\n        \n        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n        self.bn3 = nn.BatchNorm2d(128)\n        \n        # Max pooling layers\n        self.maxpool1 = nn.MaxPool2d(kernel_size=2)  # (2, 2) kernel\n        self.maxpool2 = nn.MaxPool2d(kernel_size=2)  # (2, 2) kernel\n        self.maxpool3 = nn.MaxPool2d(kernel_size=(1, 2))  # (1, 2) kernel\n        \n        # Dropout layers\n        self.dropout1 = nn.Dropout(0.3)\n        self.dropout2 = nn.Dropout(0.3)\n        \n        # Fully connected layer\n        self.fc1 = nn.Linear(128 * 8, 64)  # input features = 128 * 8\n        \n        # Bidirectional LSTM layers\n        self.lstm1 = nn.LSTM(64, 256, bidirectional=True, batch_first=True)\n        self.lstm2 = nn.LSTM(512, 256, bidirectional=True, batch_first=True)\n        \n        # Final fully connected layer\n        self.fc2 = nn.Linear(512, num_of_characters)\n        \n    def forward(self, x, check_shape= False):\n        # Convolution + BatchNorm + ReLU + MaxPool\n        x = self.maxpool1(F.relu(self.bn1(self.conv1(x))))\n        if check_shape:\n            print(f'After conv1: {x.shape}')\n        x = self.maxpool2(F.relu(self.bn2(self.conv2(x))))\n        if check_shape:\n            print(f'After conv2: {x.shape}')     \n        x = self.dropout1(x)\n        x = self.maxpool3(F.relu(self.bn3(self.conv3(x))))\n        if check_shape:\n            print(f'After conv3: {x.shape}')\n        x = self.dropout2(x)\n        # Reshape from (batch, channels, height, width) to (batch, height, channels * width)\n        batch, channels, height, width = x.size()\n        x = x.permute(0, 2, 1, 3).contiguous().view(batch, height, channels * width)\n        if check_shape:\n            print(f'After reshape: {x.shape}')   \n        # Fully connected layer\n        x = F.relu(self.fc1(x))\n        if check_shape:\n            print(f'After fc1: {x.shape}')\n        # Bidirectional LSTM layers\n        x, _ = self.lstm1(x)\n        if check_shape:\n            print(f'After lstm1: {x.shape}')\n        x, _ = self.lstm2(x)\n        if check_shape:\n            print(f'After lstm2: {x.shape}')\n        # Output layer\n        x = self.fc2(x)\n        x = F.log_softmax(x, dim=2)\n        if check_shape:\n            print(f'After fc2 (log_softmax): {x.shape}')\n        return x\n\n# Example usage\nnum_of_characters = 31  # Number of possible characters\nmodel = CNNtoRNN(num_of_characters)\n\n# Create a random tensor with the shape of your input data\ninput_tensor = torch.randn(1, 1, 256, 64)  # (batch_size, channels, height, width)\n\n# Forward pass to print the shape of each layer's output\noutput = model(input_tensor, check_shape= True)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.32735Z","iopub.execute_input":"2024-06-08T17:36:33.327686Z","iopub.status.idle":"2024-06-08T17:36:33.660362Z","shell.execute_reply.started":"2024-06-08T17:36:33.327658Z","shell.execute_reply":"2024-06-08T17:36:33.659246Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Test with a dummy input\ndummy_input = torch.randn(1 , 1, 256, 64)\noutput = model(dummy_input)\nprint(\"Final output shape: \", output.shape)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.662156Z","iopub.execute_input":"2024-06-08T17:36:33.662746Z","iopub.status.idle":"2024-06-08T17:36:33.697663Z","shell.execute_reply.started":"2024-06-08T17:36:33.662708Z","shell.execute_reply":"2024-06-08T17:36:33.696643Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Load the pretrain model\npretrain_path = \"/kaggle/input/pretrain_crnn/pytorch/65-80-epoch/1/model_80.pth\"\nmodel = torch.load(pretrain_path)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.698927Z","iopub.execute_input":"2024-06-08T17:36:33.699298Z","iopub.status.idle":"2024-06-08T17:36:33.973429Z","shell.execute_reply.started":"2024-06-08T17:36:33.699263Z","shell.execute_reply":"2024-06-08T17:36:33.972641Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device('cuda') # if torch.cuda.is_available() else 'cpu'\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:33.974479Z","iopub.execute_input":"2024-06-08T17:36:33.974784Z","iopub.status.idle":"2024-06-08T17:36:34.077421Z","shell.execute_reply.started":"2024-06-08T17:36:33.97476Z","shell.execute_reply":"2024-06-08T17:36:34.076552Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"num_epochs = 60\nlr = 0.0001\n\n# Define the loss function and optimizer\ncriterion = nn.CTCLoss(blank=0,zero_infinity=True)  # 0 is the index for the CTC blank label\noptimizer = optim.Adam(model.parameters(), lr=lr)\n\ndef calculate_input_lengths(outputs, labels):\n    # Calculate the lengths of the inputs and labels\n    input_lengths = torch.full(size=(outputs.size(1),), fill_value=outputs.size(0), dtype=torch.long)\n    label_lengths = torch.tensor([len(label) for label in labels], dtype=torch.long)\n    return input_lengths, label_lengths\n\nfor epoch in range(num_epochs):\n    model.train()\n    for batch_idx, (images, labels, label_lengths) in enumerate(train_loader):\n        images = images.to(device)\n        labels = labels.to(device)\n        label_lengths = label_lengths.to(device)\n        \n        optimizer.zero_grad()\n        \n        outputs = model(images)\n        outputs = outputs.permute(1, 0, 2)\n        \n        input_lengths, label_lengths = calculate_input_lengths(outputs, labels)\n        \n        loss = criterion(outputs, labels, input_lengths, label_lengths)\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n\n        optimizer.step()\n        \n        if (batch_idx % 100 == 0): #and not ((math.isnan(loss.item()) or math.isinf(loss.item()))):\n            print(f'Epoch {epoch+1}/{num_epochs}, Batch {batch_idx}/{len(train_loader)}, Loss: {loss.item()}')\n            if (math.isnan(loss.item()) or math.isinf(loss.item())):\n                break\n            else:\n                save_path = f\"/kaggle/working/model_{epoch+81}.pth\"\n                torch.save(model, save_path)\n    if (math.isnan(loss.item()) or math.isinf(loss.item())):\n        break        \nprint(\"Training complete.\")","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:34.078677Z","iopub.execute_input":"2024-06-08T17:36:34.078964Z","iopub.status.idle":"2024-06-08T17:36:34.084845Z","shell.execute_reply.started":"2024-06-08T17:36:34.07894Z","shell.execute_reply":"2024-06-08T17:36:34.083775Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Plot some predictions","metadata":{}},{"cell_type":"code","source":"# Decoding function\ndef greedy_decoder(output, labels):\n    output = output.cpu().numpy()\n    arg_maxes = np.argmax(output, axis=2)\n    decodes = []\n    for i in range(arg_maxes.shape[1]):\n        args = arg_maxes[:, i]\n        decode = []\n        for j in range(args.shape[0]):\n            index = args[j]\n            if index != 0:\n                decode.append(labels[index])\n        decodes.append(decode)\n    return decodes\n\nlabels = ['<BLANK>', 'A', 'B', 'C', 'D', 'E', 'F', 'G', 'H', 'I', 'J', 'K', 'L', 'M', 'N', 'O', 'P', 'Q', 'R', 'S', 'T', 'U', 'V', 'W', 'X', 'Y', 'Z', '-', \"'\", '`', ' ']","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:34.086003Z","iopub.execute_input":"2024-06-08T17:36:34.086279Z","iopub.status.idle":"2024-06-08T17:36:34.097508Z","shell.execute_reply.started":"2024-06-08T17:36:34.086256Z","shell.execute_reply":"2024-06-08T17:36:34.096507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"infer_size = 25\n# Prepare the inference DataLoader\ninference_dataset = dataset(test_img_dir, test['FILENAME'].head(infer_size).values, test['IDENTITY'].head(infer_size).values, transform=transform)\ninference_loader = DataLoader(inference_dataset, batch_size=1, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:34.098689Z","iopub.execute_input":"2024-06-08T17:36:34.098987Z","iopub.status.idle":"2024-06-08T17:36:34.109128Z","shell.execute_reply.started":"2024-06-08T17:36:34.098962Z","shell.execute_reply":"2024-06-08T17:36:34.108257Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model.to(device)\nmodel.eval()\n\nresults = []\nfor images, _, _ in inference_loader:\n    images = images.to(device)\n    with torch.no_grad():\n        outputs = model(images)\n        outputs = outputs.permute(1, 0, 2)  # Shape should be (seq_len, batch, num_of_characters)\n        outputs = F.log_softmax(outputs, dim=2)\n        decoded_output = greedy_decoder(outputs, labels)\n        results.append((images.cpu().squeeze().numpy(), ''.join(decoded_output[0])))\n\n# Plot the results\ndef plot_results(results):\n    plt.figure(figsize=(20, 10))\n    for i, (image, prediction) in enumerate(results):\n        plt.subplot(5, 5, i + 1)  # Adjust subplot size according to number of results\n        image = cv2.rotate(image, cv2.ROTATE_90_COUNTERCLOCKWISE)\n        plt.imshow(image, cmap='gray')\n        plt.title(prediction)\n        plt.axis('off')\n    plt.show()\n\n# Show the first 25 results\nplot_results(results[:25])","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:34.110323Z","iopub.execute_input":"2024-06-08T17:36:34.110674Z","iopub.status.idle":"2024-06-08T17:36:36.50371Z","shell.execute_reply.started":"2024-06-08T17:36:34.110649Z","shell.execute_reply":"2024-06-08T17:36:36.50271Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate on validate set","metadata":{}},{"cell_type":"code","source":"# Function to calculate character-level accuracy\ndef character_level_accuracy(preds, targets):\n    correct_characters = 0\n    total_characters = 0\n    for pred, target in zip(preds, targets):\n        total_characters += len(target)\n        correct_characters += sum(p == t for p, t in zip(pred, target))\n    accuracy = correct_characters / total_characters if total_characters > 0 else 0\n    return accuracy * 100\n\n# Function to calculate word-level accuracy\ndef word_level_accuracy(preds, targets):\n    correct_words = sum(p == t for p, t in zip(preds, targets))\n    total_words = len(targets)\n    accuracy = correct_words / total_words if total_words > 0 else 0\n    return accuracy * 100","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:36.504842Z","iopub.execute_input":"2024-06-08T17:36:36.505153Z","iopub.status.idle":"2024-06-08T17:36:36.512197Z","shell.execute_reply.started":"2024-06-08T17:36:36.505128Z","shell.execute_reply":"2024-06-08T17:36:36.511249Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(valid)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:36.513672Z","iopub.execute_input":"2024-06-08T17:36:36.514059Z","iopub.status.idle":"2024-06-08T17:36:36.524562Z","shell.execute_reply.started":"2024-06-08T17:36:36.514026Z","shell.execute_reply":"2024-06-08T17:36:36.523675Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the val DataLoader\nval_size = int(len(valid))\nvalidation_dataset = dataset(val_img_dir, valid['FILENAME'].head(val_size).values, valid['IDENTITY'].head(val_size).values, transform=transform)\nvalidation_loader = DataLoader(validation_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:36.529687Z","iopub.execute_input":"2024-06-08T17:36:36.529968Z","iopub.status.idle":"2024-06-08T17:36:36.536404Z","shell.execute_reply.started":"2024-06-08T17:36:36.529945Z","shell.execute_reply":"2024-06-08T17:36:36.535481Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on validation set\nmodel.eval()\nall_predictions = []\nall_targets = []\nwith torch.no_grad():\n    for images, targets, _ in validation_loader:\n        images = images.to(device)\n        outputs = model(images)\n        outputs = outputs.permute(1, 0, 2)  # Shape should be (seq_len, batch, num_of_characters)\n        outputs = F.log_softmax(outputs, dim=2)\n        decoded_output = greedy_decoder(outputs, labels)\n        \n        # Ensure targets are in the correct format\n        target_strings = []\n        for target in targets:\n            if isinstance(target, torch.Tensor):\n                idx = target[torch.nonzero(target)]\n#                 print(idx.reshape(-1).tolist())\n                target_strings.append(''.join([labels[x] for x in idx]))\n            else:\n                target_strings.append(''.join([labels[char] for char in target if char != 0]))\n        \n        all_predictions.extend([''.join(dec) for dec in decoded_output])\n        all_targets.extend(target_strings)\n\nchar_accuracy = character_level_accuracy(all_predictions, all_targets)\nword_accuracy = word_level_accuracy(all_predictions, all_targets)\n\nprint(f'Correct characters predicted : {char_accuracy:.2f}%')\nprint(f'Correct words predicted      : {word_accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:36:36.53775Z","iopub.execute_input":"2024-06-08T17:36:36.538129Z","iopub.status.idle":"2024-06-08T17:43:35.247077Z","shell.execute_reply.started":"2024-06-08T17:36:36.538097Z","shell.execute_reply":"2024-06-08T17:43:35.245974Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Evaluate on test set","metadata":{}},{"cell_type":"code","source":"test.dropna(axis=0, inplace=True)\ntest = test[test['IDENTITY'] != 'UNREADABLE']\ntest = test[test['IDENTITY'] != 'EMPTY']\ntest['IDENTITY'] = test['IDENTITY'].str.upper()\ntest.reset_index(inplace = True, drop=True)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:43:35.24811Z","iopub.execute_input":"2024-06-08T17:43:35.248366Z","iopub.status.idle":"2024-06-08T17:43:35.299334Z","shell.execute_reply.started":"2024-06-08T17:43:35.248344Z","shell.execute_reply":"2024-06-08T17:43:35.298289Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(test)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:43:35.300812Z","iopub.execute_input":"2024-06-08T17:43:35.301096Z","iopub.status.idle":"2024-06-08T17:43:35.307336Z","shell.execute_reply.started":"2024-06-08T17:43:35.301073Z","shell.execute_reply":"2024-06-08T17:43:35.306243Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Prepare the test DataLoader\ntest_size = int(len(test))\ntest_dataset = dataset(test_img_dir, test['FILENAME'].head(test_size).values, test['IDENTITY'].head(test_size).values, transform=transform)\ntest_loader = DataLoader(test_dataset, batch_size=128, shuffle=False, collate_fn=collate_fn)","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:43:35.308629Z","iopub.execute_input":"2024-06-08T17:43:35.309256Z","iopub.status.idle":"2024-06-08T17:43:35.315848Z","shell.execute_reply.started":"2024-06-08T17:43:35.30923Z","shell.execute_reply":"2024-06-08T17:43:35.31477Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Evaluate on test set\nmodel.eval()\nall_predictions = []\nall_targets = []\nwith torch.no_grad():\n    for images, targets, _ in test_loader:\n        images = images.to(device)\n        outputs = model(images)\n        outputs = outputs.permute(1, 0, 2)  # Shape should be (seq_len, batch, num_of_characters)\n        outputs = F.log_softmax(outputs, dim=2)\n        decoded_output = greedy_decoder(outputs, labels)\n        \n        # Ensure targets are in the correct format\n        target_strings = []\n        for target in targets:\n            if isinstance(target, torch.Tensor):\n                idx = target[torch.nonzero(target)]\n#                 print(idx.reshape(-1).tolist())\n                target_strings.append(''.join([labels[x] for x in idx]))\n            else:\n                target_strings.append(''.join([labels[char] for char in target if char != 0]))\n        \n        all_predictions.extend([''.join(dec) for dec in decoded_output])\n        all_targets.extend(target_strings)\n\nchar_accuracy = character_level_accuracy(all_predictions, all_targets)\nword_accuracy = word_level_accuracy(all_predictions, all_targets)\n\nprint(f'Correct characters predicted : {char_accuracy:.2f}%')\nprint(f'Correct words predicted      : {word_accuracy:.2f}%')","metadata":{"execution":{"iopub.status.busy":"2024-06-08T17:43:35.317118Z","iopub.execute_input":"2024-06-08T17:43:35.317487Z","iopub.status.idle":"2024-06-08T17:50:54.94471Z","shell.execute_reply.started":"2024-06-08T17:43:35.317454Z","shell.execute_reply":"2024-06-08T17:50:54.943816Z"},"trusted":true},"execution_count":null,"outputs":[]}]}